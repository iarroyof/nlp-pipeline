# Author: Ignacio Arroyo-Fernandez (UNAM)

from gensim.models import Word2Vec
import os
from argparse import ArgumentParser as ap
import sys

def clean_Ustring_fromU(string):
    from unicodedata import name, normalize
    gClean = ''
    for ch in u''.join(string.decode('utf-8', 'ignore')):
        try:
            if name(ch).startswith('LATIN') or name(ch) == 'SPACE':
                gClean = gClean + ch
            else: # Remove non-latin characters and change them by spaces
                gClean = gClean + ' '
        except ValueError: # In the case name of 'ch' does not exist in the unicode database.
            gClean = gClean + ' '
    
    try: # Trying different cases for bad input documents.
        normalized_string = normalize('NFKC', gClean.lower())
    except TypeError:
        sys.stderr.write('The wrong string at the first attempt\n')
        try:
            range_error = 999
            normalized_string = normalize('NFKC', gClean[0:range_error].lower()) # One thousand of characters are written if available. 
        except TypeError:
            sys.stderr.write('\nThe wrong string at the second attempt: before %s words' % range_error)
            try:
                range_error = 99
                normalized_string = normalize('NFKC', gClean[0:range_error].lower())
            except TypeError:
                sys.stderr.write('\nThe wrong string at the third attempt: before %s words' % range_error)
                try:
                    range_error = 49
                    normalized_string = normalize('NFKC', gClean[0:range_error].lower())
                except TypeError:    
                    sys.stderr.write('\nIt was not possible forming output file after three attempts. Fatally bad file')
                    normalized_string = '# Fatally bad File\n'
                    pass
    return  normalized_string # Return the unicode normalized document.

class yield_line_documents(object):
    def __init__(self, dirname):
        self.dirname = dirname
    
    def __iter__(self):
        
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield clean_Ustring_fromU(line).split()

if __name__ == "__main__":
    parser = ap(description='Trains and saves a word2vec model into a file for mmap\'ing. Tokenization is performed un utf-8 an for Python 2.7. Non-latin characters are replaced by spaces. The model is saved into a given directory. All options are needed.')    
    parser.add_argument('-i', type=str, dest = 'indirname', help='Specifies the directory containing files to be processed. No sub-directories are allowed.')
    parser.add_argument('-o', type=str, dest = 'outfile', help='Specifies the file where to be stored the model.')
    parser.add_argument('-t', type=int, dest = 'threads', help='Specifies the number of threads the training will be divided.')
    parser.add_argument('-H', type=int, dest = 'hidden', help='Specifies the number of hidden units the model going to have.')
    parser.add_argument('-m', type=int, dest = 'minc', help='Specifies the minimum frequency a word should have in the corpus to be considered.')
    #args_indirname = '/home/ignacio/WikiFr_cadenas_test/'
    #args_outfile = '/home/ignacio/w2v_model/mymodel'
    #args_threads = 4
    #args_hidden = 100
    #args_minc = 5
    args = parser.parse_args()
    articles = yield_line_documents(args.indirname)
    w2v_model = Word2Vec(articles, min_count = args.minc, workers = args.threads, size = args.hidden)
    w2v_model.save(args.outfile, separately = None)
    
    #w2v_new = Word2Vec.load(args_outfile)
    #print w2v_new['computer']
    	
