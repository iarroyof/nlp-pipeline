ignacio@np13:~$ python $NLP/attention_lstm_1.py
Using Theano backend.
Loanding train and valid dirs......
Starting training
Phrases in STS.input.FNWN.txt 189 189
Phrases in STS.input.OnWN.txt 561 561
Phrases in STS.input.headlines.txt 750 750
Total train phrases /almac/ignacio/data/sts_all/train-2013 1500
Total train phrases 3000
Starting training
Phrases in STS.input.track5.en-en.txt 250
Total train phrases /almac/ignacio/data/sts_all/valid-2017 250
Total train phrases 500
Spliting tab-separated files...
Labels shape:  (1500,)
Tokenizing files... [A]
Split training set into train and val... [A]
Tokenizing files... [B]
Split training set into train and val... [B]
Getting embedding matrix...
Found 4049540 word vectors.
Filling embedding matrices...
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
embedding_1 (Embedding)          (None, 50, 50)        185950                                       
____________________________________________________________________________________________________
attention_1 (Attention)          (None, 50, 25)        8900                                         
____________________________________________________________________________________________________
attention_2 (Attention)          (None, 50, 25)        2575                                         
____________________________________________________________________________________________________
attention_3 (Attention)          (None, 50, 25)        2575                                         
____________________________________________________________________________________________________
attention_4 (Attention)          (None, 25)            2575                                         
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 25)            0                                            
____________________________________________________________________________________________________
embedding_2 (Embedding)          (None, 50, 50)        212200                                       
____________________________________________________________________________________________________
attention_5 (Attention)          (None, 50, 25)        8900                                         
____________________________________________________________________________________________________
attention_6 (Attention)          (None, 50, 25)        2575                                         
____________________________________________________________________________________________________
attention_7 (Attention)          (None, 50, 25)        2575                                         
____________________________________________________________________________________________________
attention_8 (Attention)          (None, 25)            2575                                         
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 25)            0                                            
____________________________________________________________________________________________________
maxoutdense_1 (MaxoutDense)      (None, 10)            2040        merge_1[0][0]                    
____________________________________________________________________________________________________
maxoutdense_2 (MaxoutDense)      (None, 1)             44          maxoutdense_1[0][0]              
====================================================================================================
Total params: 433,484
Trainable params: 35,334
Non-trainable params: 398,150
____________________________________________________________________________________________________
None
Train on 1050 samples, validate on 450 samples
Epoch 1/100
1050/1050 [==============================] - 193s - loss: 1.4472 - mean_absolute_error: 1.4472 - mean_squared_error: 2.7143 - val_loss: 1.3324 - val_mean_absolute_error: 1.3324 - val_mean_squared_error: 2.2458
Epoch 2/100
1050/1050 [==============================] - 192s - loss: 1.4084 - mean_absolute_error: 1.4084 - mean_squared_error: 2.5455 - val_loss: 1.3294 - val_mean_absolute_error: 1.3294 - val_mean_squared_error: 2.2452
Epoch 3/100
1050/1050 [==============================] - 196s - loss: 1.4026 - mean_absolute_error: 1.4026 - mean_squared_error: 2.5020 - val_loss: 1.3339 - val_mean_absolute_error: 1.3339 - val_mean_squared_error: 2.3053
Epoch 4/100
1050/1050 [==============================] - 195s - loss: 1.3939 - mean_absolute_error: 1.3939 - mean_squared_error: 2.5028 - val_loss: 1.3022 - val_mean_absolute_error: 1.3022 - val_mean_squared_error: 2.1776
Epoch 5/100
1050/1050 [==============================] - 193s - loss: 1.3988 - mean_absolute_error: 1.3988 - mean_squared_error: 2.5061 - val_loss: 1.2848 - val_mean_absolute_error: 1.2848 - val_mean_squared_error: 2.1189
Epoch 6/100
1050/1050 [==============================] - 191s - loss: 1.3460 - mean_absolute_error: 1.3460 - mean_squared_error: 2.4301 - val_loss: 1.2132 - val_mean_absolute_error: 1.2132 - val_mean_squared_error: 1.9379
Epoch 7/100
1050/1050 [==============================] - 195s - loss: 1.3257 - mean_absolute_error: 1.3257 - mean_squared_error: 2.3989 - val_loss: 1.2495 - val_mean_absolute_error: 1.2495 - val_mean_squared_error: 2.1899
Epoch 8/100
1050/1050 [==============================] - 195s - loss: 1.2884 - mean_absolute_error: 1.2884 - mean_squared_error: 2.3543 - val_loss: 1.3250 - val_mean_absolute_error: 1.3250 - val_mean_squared_error: 2.5787
Epoch 9/100
1050/1050 [==============================] - 201s - loss: 1.3130 - mean_absolute_error: 1.3130 - mean_squared_error: 2.3793 - val_loss: 1.1966 - val_mean_absolute_error: 1.1966 - val_mean_squared_error: 1.9480
Epoch 10/100
1050/1050 [==============================] - 195s - loss: 1.2762 - mean_absolute_error: 1.2762 - mean_squared_error: 2.3427 - val_loss: 1.1831 - val_mean_absolute_error: 1.1831 - val_mean_squared_error: 1.9553
Epoch 11/100
1050/1050 [==============================] - 203s - loss: 1.2525 - mean_absolute_error: 1.2525 - mean_squared_error: 2.2505 - val_loss: 1.1663 - val_mean_absolute_error: 1.1663 - val_mean_squared_error: 1.8784
Epoch 12/100
1050/1050 [==============================] - 203s - loss: 1.2791 - mean_absolute_error: 1.2791 - mean_squared_error: 2.4150 - val_loss: 1.1392 - val_mean_absolute_error: 1.1392 - val_mean_squared_error: 1.9144
Epoch 13/100
1050/1050 [==============================] - 195s - loss: 1.2549 - mean_absolute_error: 1.2549 - mean_squared_error: 2.2776 - val_loss: 1.1468 - val_mean_absolute_error: 1.1468 - val_mean_squared_error: 1.9456
Epoch 14/100
1050/1050 [==============================] - 193s - loss: 1.2325 - mean_absolute_error: 1.2325 - mean_squared_error: 2.2474 - val_loss: 1.1419 - val_mean_absolute_error: 1.1419 - val_mean_squared_error: 1.8419
Epoch 15/100
1050/1050 [==============================] - 197s - loss: 1.2381 - mean_absolute_error: 1.2381 - mean_squared_error: 2.2374 - val_loss: 1.1283 - val_mean_absolute_error: 1.1283 - val_mean_squared_error: 1.8320
Epoch 16/100
1050/1050 [==============================] - 201s - loss: 1.2248 - mean_absolute_error: 1.2248 - mean_squared_error: 2.2236 - val_loss: 1.1298 - val_mean_absolute_error: 1.1298 - val_mean_squared_error: 1.9232
Epoch 17/100
1050/1050 [==============================] - 199s - loss: 1.2246 - mean_absolute_error: 1.2246 - mean_squared_error: 2.2893 - val_loss: 1.2010 - val_mean_absolute_error: 1.2010 - val_mean_squared_error: 2.1447
Epoch 18/100
1050/1050 [==============================] - 196s - loss: 1.2175 - mean_absolute_error: 1.2175 - mean_squared_error: 2.1963 - val_loss: 1.1176 - val_mean_absolute_error: 1.1176 - val_mean_squared_error: 1.8073
Epoch 19/100
1050/1050 [==============================] - 199s - loss: 1.2114 - mean_absolute_error: 1.2114 - mean_squared_error: 2.1534 - val_loss: 1.1765 - val_mean_absolute_error: 1.1765 - val_mean_squared_error: 2.0621
Epoch 20/100
1050/1050 [==============================] - 195s - loss: 1.2109 - mean_absolute_error: 1.2109 - mean_squared_error: 2.2463 - val_loss: 1.1313 - val_mean_absolute_error: 1.1313 - val_mean_squared_error: 1.9421
Epoch 21/100
1050/1050 [==============================] - 195s - loss: 1.1793 - mean_absolute_error: 1.1793 - mean_squared_error: 2.1221 - val_loss: 1.1137 - val_mean_absolute_error: 1.1137 - val_mean_squared_error: 2.0367
Epoch 22/100
1050/1050 [==============================] - 197s - loss: 1.2094 - mean_absolute_error: 1.2094 - mean_squared_error: 2.2578 - val_loss: 1.1141 - val_mean_absolute_error: 1.1141 - val_mean_squared_error: 1.7991
Epoch 23/100
1050/1050 [==============================] - 194s - loss: 1.1895 - mean_absolute_error: 1.1895 - mean_squared_error: 2.1457 - val_loss: 1.1428 - val_mean_absolute_error: 1.1428 - val_mean_squared_error: 2.1624
Epoch 24/100
1050/1050 [==============================] - 194s - loss: 1.1438 - mean_absolute_error: 1.1438 - mean_squared_error: 2.0956 - val_loss: 1.1197 - val_mean_absolute_error: 1.1197 - val_mean_squared_error: 1.8958
Epoch 25/100
1050/1050 [==============================] - 196s - loss: 1.1587 - mean_absolute_error: 1.1587 - mean_squared_error: 2.0661 - val_loss: 1.1269 - val_mean_absolute_error: 1.1269 - val_mean_squared_error: 1.9923
Epoch 26/100
1050/1050 [==============================] - 198s - loss: 1.1695 - mean_absolute_error: 1.1695 - mean_squared_error: 2.1069 - val_loss: 1.1239 - val_mean_absolute_error: 1.1239 - val_mean_squared_error: 2.1451
Epoch 27/100
1050/1050 [==============================] - 195s - loss: 1.1167 - mean_absolute_error: 1.1167 - mean_squared_error: 1.9691 - val_loss: 1.1244 - val_mean_absolute_error: 1.1244 - val_mean_squared_error: 2.0383
Epoch 28/100
1050/1050 [==============================] - 194s - loss: 1.1432 - mean_absolute_error: 1.1432 - mean_squared_error: 2.1146 - val_loss: 1.1609 - val_mean_absolute_error: 1.1609 - val_mean_squared_error: 2.0065
Epoch 29/100
1050/1050 [==============================] - 197s - loss: 1.1411 - mean_absolute_error: 1.1411 - mean_squared_error: 2.0925 - val_loss: 1.1149 - val_mean_absolute_error: 1.1149 - val_mean_squared_error: 1.9947
Epoch 30/100
1050/1050 [==============================] - 198s - loss: 1.1431 - mean_absolute_error: 1.1431 - mean_squared_error: 2.0592 - val_loss: 1.1064 - val_mean_absolute_error: 1.1064 - val_mean_squared_error: 1.9542
Epoch 31/100
1050/1050 [==============================] - 201s - loss: 1.1359 - mean_absolute_error: 1.1359 - mean_squared_error: 2.0429 - val_loss: 1.1422 - val_mean_absolute_error: 1.1422 - val_mean_squared_error: 2.1531
Epoch 32/100
1050/1050 [==============================] - 207s - loss: 1.1060 - mean_absolute_error: 1.1060 - mean_squared_error: 1.9652 - val_loss: 1.1476 - val_mean_absolute_error: 1.1476 - val_mean_squared_error: 2.1878
Epoch 33/100
1050/1050 [==============================] - 203s - loss: 1.1169 - mean_absolute_error: 1.1169 - mean_squared_error: 1.9998 - val_loss: 1.1204 - val_mean_absolute_error: 1.1204 - val_mean_squared_error: 1.9927
Epoch 34/100
1050/1050 [==============================] - 200s - loss: 1.1279 - mean_absolute_error: 1.1279 - mean_squared_error: 2.0551 - val_loss: 1.1257 - val_mean_absolute_error: 1.1257 - val_mean_squared_error: 2.0321
Epoch 35/100
1050/1050 [==============================] - 197s - loss: 1.0936 - mean_absolute_error: 1.0936 - mean_squared_error: 1.9594 - val_loss: 1.1445 - val_mean_absolute_error: 1.1445 - val_mean_squared_error: 2.2837
Epoch 36/100
1050/1050 [==============================] - 196s - loss: 1.1026 - mean_absolute_error: 1.1026 - mean_squared_error: 2.0052 - val_loss: 1.1274 - val_mean_absolute_error: 1.1274 - val_mean_squared_error: 1.9724
Epoch 37/100
1050/1050 [==============================] - 200s - loss: 1.0919 - mean_absolute_error: 1.0919 - mean_squared_error: 1.9625 - val_loss: 1.1505 - val_mean_absolute_error: 1.1505 - val_mean_squared_error: 2.0624
Epoch 38/100
1050/1050 [==============================] - 197s - loss: 1.0703 - mean_absolute_error: 1.0703 - mean_squared_error: 1.8320 - val_loss: 1.1972 - val_mean_absolute_error: 1.1972 - val_mean_squared_error: 2.3799
Epoch 39/100
1050/1050 [==============================] - 195s - loss: 1.0872 - mean_absolute_error: 1.0872 - mean_squared_error: 1.9688 - val_loss: 1.1677 - val_mean_absolute_error: 1.1677 - val_mean_squared_error: 2.1061
Epoch 40/100
1050/1050 [==============================] - 194s - loss: 1.0855 - mean_absolute_error: 1.0855 - mean_squared_error: 1.9955 - val_loss: 1.1008 - val_mean_absolute_error: 1.1008 - val_mean_squared_error: 1.9622
Epoch 41/100
1050/1050 [==============================] - 194s - loss: 1.0770 - mean_absolute_error: 1.0770 - mean_squared_error: 1.9100 - val_loss: 1.1566 - val_mean_absolute_error: 1.1566 - val_mean_squared_error: 2.1571
Epoch 42/100
1050/1050 [==============================] - 195s - loss: 1.0699 - mean_absolute_error: 1.0699 - mean_squared_error: 1.9204 - val_loss: 1.1246 - val_mean_absolute_error: 1.1246 - val_mean_squared_error: 1.9581
Epoch 43/100
1050/1050 [==============================] - 194s - loss: 1.0509 - mean_absolute_error: 1.0509 - mean_squared_error: 1.8357 - val_loss: 1.1475 - val_mean_absolute_error: 1.1475 - val_mean_squared_error: 2.2722
Epoch 44/100
1050/1050 [==============================] - 194s - loss: 1.0604 - mean_absolute_error: 1.0604 - mean_squared_error: 1.9418 - val_loss: 1.1514 - val_mean_absolute_error: 1.1514 - val_mean_squared_error: 2.1891
Epoch 45/100
1050/1050 [==============================] - 194s - loss: 1.0264 - mean_absolute_error: 1.0264 - mean_squared_error: 1.8166 - val_loss: 1.1348 - val_mean_absolute_error: 1.1348 - val_mean_squared_error: 2.1075
Epoch 46/100
1050/1050 [==============================] - 193s - loss: 1.0326 - mean_absolute_error: 1.0326 - mean_squared_error: 1.8264 - val_loss: 1.1928 - val_mean_absolute_error: 1.1928 - val_mean_squared_error: 2.3400
Epoch 47/100
1050/1050 [==============================] - 197s - loss: 1.0298 - mean_absolute_error: 1.0298 - mean_squared_error: 1.8175 - val_loss: 1.1663 - val_mean_absolute_error: 1.1663 - val_mean_squared_error: 2.2285
Epoch 48/100
1050/1050 [==============================] - 197s - loss: 1.0493 - mean_absolute_error: 1.0493 - mean_squared_error: 1.8643 - val_loss: 1.1652 - val_mean_absolute_error: 1.1652 - val_mean_squared_error: 2.3336
Epoch 49/100
1050/1050 [==============================] - 194s - loss: 1.0439 - mean_absolute_error: 1.0439 - mean_squared_error: 1.8112 - val_loss: 1.1462 - val_mean_absolute_error: 1.1462 - val_mean_squared_error: 2.1684
Epoch 50/100
1050/1050 [==============================] - 194s - loss: 1.0393 - mean_absolute_error: 1.0393 - mean_squared_error: 1.8862 - val_loss: 1.1698 - val_mean_absolute_error: 1.1698 - val_mean_squared_error: 2.3521
Epoch 51/100
1050/1050 [==============================] - 194s - loss: 1.0244 - mean_absolute_error: 1.0244 - mean_squared_error: 1.8186 - val_loss: 1.1566 - val_mean_absolute_error: 1.1566 - val_mean_squared_error: 2.2483
Epoch 52/100
1050/1050 [==============================] - 196s - loss: 1.0264 - mean_absolute_error: 1.0264 - mean_squared_error: 1.8264 - val_loss: 1.1281 - val_mean_absolute_error: 1.1281 - val_mean_squared_error: 2.1981
Epoch 53/100
1050/1050 [==============================] - 194s - loss: 0.9788 - mean_absolute_error: 0.9788 - mean_squared_error: 1.6856 - val_loss: 1.1805 - val_mean_absolute_error: 1.1805 - val_mean_squared_error: 2.3246
Epoch 54/100
1050/1050 [==============================] - 193s - loss: 0.9980 - mean_absolute_error: 0.9980 - mean_squared_error: 1.7019 - val_loss: 1.1164 - val_mean_absolute_error: 1.1164 - val_mean_squared_error: 2.0692
Epoch 55/100
1050/1050 [==============================] - 194s - loss: 0.9960 - mean_absolute_error: 0.9960 - mean_squared_error: 1.7555 - val_loss: 1.1628 - val_mean_absolute_error: 1.1628 - val_mean_squared_error: 2.3504
Epoch 56/100
1050/1050 [==============================] - 193s - loss: 1.0177 - mean_absolute_error: 1.0177 - mean_squared_error: 1.8193 - val_loss: 1.2132 - val_mean_absolute_error: 1.2132 - val_mean_squared_error: 2.5549
Epoch 57/100
1050/1050 [==============================] - 194s - loss: 0.9938 - mean_absolute_error: 0.9938 - mean_squared_error: 1.7733 - val_loss: 1.2112 - val_mean_absolute_error: 1.2112 - val_mean_squared_error: 2.5557
Epoch 58/100
1050/1050 [==============================] - 192s - loss: 1.0141 - mean_absolute_error: 1.0141 - mean_squared_error: 1.8466 - val_loss: 1.1634 - val_mean_absolute_error: 1.1634 - val_mean_squared_error: 2.3602
Epoch 59/100
1050/1050 [==============================] - 193s - loss: 0.9907 - mean_absolute_error: 0.9907 - mean_squared_error: 1.6806 - val_loss: 1.2531 - val_mean_absolute_error: 1.2531 - val_mean_squared_error: 2.7095
Epoch 60/100
1050/1050 [==============================] - 194s - loss: 0.9980 - mean_absolute_error: 0.9980 - mean_squared_error: 1.7786 - val_loss: 1.1483 - val_mean_absolute_error: 1.1483 - val_mean_squared_error: 2.1379
Epoch 61/100
1050/1050 [==============================] - 194s - loss: 0.9703 - mean_absolute_error: 0.9703 - mean_squared_error: 1.6716 - val_loss: 1.1238 - val_mean_absolute_error: 1.1238 - val_mean_squared_error: 2.0950
Epoch 62/100
1050/1050 [==============================] - 195s - loss: 0.9618 - mean_absolute_error: 0.9618 - mean_squared_error: 1.6845 - val_loss: 1.1722 - val_mean_absolute_error: 1.1722 - val_mean_squared_error: 2.3699
Epoch 63/100
1050/1050 [==============================] - 194s - loss: 0.9534 - mean_absolute_error: 0.9534 - mean_squared_error: 1.6555 - val_loss: 1.1623 - val_mean_absolute_error: 1.1623 - val_mean_squared_error: 2.2428
Epoch 64/100
1050/1050 [==============================] - 195s - loss: 0.9553 - mean_absolute_error: 0.9553 - mean_squared_error: 1.6038 - val_loss: 1.1935 - val_mean_absolute_error: 1.1935 - val_mean_squared_error: 2.4924
Epoch 65/100
1050/1050 [==============================] - 195s - loss: 0.9979 - mean_absolute_error: 0.9979 - mean_squared_error: 1.7466 - val_loss: 1.1682 - val_mean_absolute_error: 1.1682 - val_mean_squared_error: 2.3832
Epoch 66/100
1050/1050 [==============================] - 195s - loss: 0.9643 - mean_absolute_error: 0.9643 - mean_squared_error: 1.6776 - val_loss: 1.1663 - val_mean_absolute_error: 1.1663 - val_mean_squared_error: 2.3274
Epoch 67/100
1050/1050 [==============================] - 195s - loss: 0.9715 - mean_absolute_error: 0.9715 - mean_squared_error: 1.6804 - val_loss: 1.1842 - val_mean_absolute_error: 1.1842 - val_mean_squared_error: 2.3672
Epoch 68/100
1050/1050 [==============================] - 194s - loss: 0.9622 - mean_absolute_error: 0.9622 - mean_squared_error: 1.6285 - val_loss: 1.1403 - val_mean_absolute_error: 1.1403 - val_mean_squared_error: 2.1159
Epoch 69/100
1050/1050 [==============================] - 194s - loss: 0.9618 - mean_absolute_error: 0.9618 - mean_squared_error: 1.6604 - val_loss: 1.1310 - val_mean_absolute_error: 1.1310 - val_mean_squared_error: 2.1934
Epoch 70/100
1050/1050 [==============================] - 195s - loss: 0.9145 - mean_absolute_error: 0.9145 - mean_squared_error: 1.5906 - val_loss: 1.2025 - val_mean_absolute_error: 1.2025 - val_mean_squared_error: 2.4112
Epoch 71/100
1050/1050 [==============================] - 194s - loss: 0.9516 - mean_absolute_error: 0.9516 - mean_squared_error: 1.7187 - val_loss: 1.1246 - val_mean_absolute_error: 1.1246 - val_mean_squared_error: 2.1474
Epoch 72/100
1050/1050 [==============================] - 197s - loss: 0.9121 - mean_absolute_error: 0.9121 - mean_squared_error: 1.5745 - val_loss: 1.2023 - val_mean_absolute_error: 1.2023 - val_mean_squared_error: 2.4598
Epoch 73/100
1050/1050 [==============================] - 194s - loss: 0.9337 - mean_absolute_error: 0.9337 - mean_squared_error: 1.6140 - val_loss: 1.1292 - val_mean_absolute_error: 1.1292 - val_mean_squared_error: 2.1803
Epoch 74/100
1050/1050 [==============================] - 194s - loss: 0.9239 - mean_absolute_error: 0.9239 - mean_squared_error: 1.5726 - val_loss: 1.1369 - val_mean_absolute_error: 1.1369 - val_mean_squared_error: 2.1370
Epoch 75/100
1050/1050 [==============================] - 194s - loss: 0.9338 - mean_absolute_error: 0.9338 - mean_squared_error: 1.5977 - val_loss: 1.1425 - val_mean_absolute_error: 1.1425 - val_mean_squared_error: 2.1418
Epoch 76/100
1050/1050 [==============================] - 195s - loss: 0.9289 - mean_absolute_error: 0.9289 - mean_squared_error: 1.5910 - val_loss: 1.1339 - val_mean_absolute_error: 1.1339 - val_mean_squared_error: 2.1815
Epoch 77/100
1050/1050 [==============================] - 196s - loss: 0.9092 - mean_absolute_error: 0.9092 - mean_squared_error: 1.5318 - val_loss: 1.1810 - val_mean_absolute_error: 1.1810 - val_mean_squared_error: 2.4074
Epoch 78/100
1050/1050 [==============================] - 194s - loss: 0.9073 - mean_absolute_error: 0.9073 - mean_squared_error: 1.5186 - val_loss: 1.2393 - val_mean_absolute_error: 1.2393 - val_mean_squared_error: 2.5613
Epoch 79/100
1050/1050 [==============================] - 194s - loss: 0.9101 - mean_absolute_error: 0.9101 - mean_squared_error: 1.5726 - val_loss: 1.1647 - val_mean_absolute_error: 1.1647 - val_mean_squared_error: 2.3648
Epoch 80/100
1050/1050 [==============================] - 197s - loss: 0.9203 - mean_absolute_error: 0.9203 - mean_squared_error: 1.5967 - val_loss: 1.1399 - val_mean_absolute_error: 1.1399 - val_mean_squared_error: 2.2131
Epoch 81/100
1050/1050 [==============================] - 194s - loss: 0.9248 - mean_absolute_error: 0.9248 - mean_squared_error: 1.5473 - val_loss: 1.2048 - val_mean_absolute_error: 1.2048 - val_mean_squared_error: 2.4625
Epoch 82/100
1050/1050 [==============================] - 195s - loss: 0.9031 - mean_absolute_error: 0.9031 - mean_squared_error: 1.5464 - val_loss: 1.1701 - val_mean_absolute_error: 1.1701 - val_mean_squared_error: 2.4762
Epoch 83/100
1050/1050 [==============================] - 193s - loss: 0.9058 - mean_absolute_error: 0.9058 - mean_squared_error: 1.5389 - val_loss: 1.1684 - val_mean_absolute_error: 1.1684 - val_mean_squared_error: 2.2155
Epoch 84/100
1050/1050 [==============================] - 194s - loss: 0.9156 - mean_absolute_error: 0.9156 - mean_squared_error: 1.5462 - val_loss: 1.1750 - val_mean_absolute_error: 1.1750 - val_mean_squared_error: 2.2269
Epoch 85/100
1050/1050 [==============================] - 196s - loss: 0.8895 - mean_absolute_error: 0.8895 - mean_squared_error: 1.4947 - val_loss: 1.1803 - val_mean_absolute_error: 1.1803 - val_mean_squared_error: 2.3331
Epoch 86/100
1050/1050 [==============================] - 196s - loss: 0.9287 - mean_absolute_error: 0.9287 - mean_squared_error: 1.5775 - val_loss: 1.1858 - val_mean_absolute_error: 1.1858 - val_mean_squared_error: 2.4515
Epoch 87/100
1050/1050 [==============================] - 196s - loss: 0.8972 - mean_absolute_error: 0.8972 - mean_squared_error: 1.5451 - val_loss: 1.2210 - val_mean_absolute_error: 1.2210 - val_mean_squared_error: 2.6368
Epoch 88/100
1050/1050 [==============================] - 196s - loss: 0.8691 - mean_absolute_error: 0.8691 - mean_squared_error: 1.4523 - val_loss: 1.2637 - val_mean_absolute_error: 1.2637 - val_mean_squared_error: 2.6947
Epoch 89/100
1050/1050 [==============================] - 194s - loss: 0.9235 - mean_absolute_error: 0.9235 - mean_squared_error: 1.6097 - val_loss: 1.1868 - val_mean_absolute_error: 1.1868 - val_mean_squared_error: 2.4712
Epoch 90/100
1050/1050 [==============================] - 196s - loss: 0.8931 - mean_absolute_error: 0.8931 - mean_squared_error: 1.5160 - val_loss: 1.1359 - val_mean_absolute_error: 1.1359 - val_mean_squared_error: 2.1496
Epoch 91/100
1050/1050 [==============================] - 196s - loss: 0.8995 - mean_absolute_error: 0.8995 - mean_squared_error: 1.5244 - val_loss: 1.1569 - val_mean_absolute_error: 1.1569 - val_mean_squared_error: 2.1702
Epoch 92/100
1050/1050 [==============================] - 197s - loss: 0.8394 - mean_absolute_error: 0.8394 - mean_squared_error: 1.3711 - val_loss: 1.1432 - val_mean_absolute_error: 1.1432 - val_mean_squared_error: 2.2423
Epoch 93/100
1050/1050 [==============================] - 195s - loss: 0.8636 - mean_absolute_error: 0.8636 - mean_squared_error: 1.4125 - val_loss: 1.2076 - val_mean_absolute_error: 1.2076 - val_mean_squared_error: 2.4945
Epoch 94/100
1050/1050 [==============================] - 196s - loss: 0.8906 - mean_absolute_error: 0.8906 - mean_squared_error: 1.5270 - val_loss: 1.1468 - val_mean_absolute_error: 1.1468 - val_mean_squared_error: 2.2323
Epoch 95/100
1050/1050 [==============================] - 196s - loss: 0.8772 - mean_absolute_error: 0.8772 - mean_squared_error: 1.4942 - val_loss: 1.2336 - val_mean_absolute_error: 1.2336 - val_mean_squared_error: 2.5533
Epoch 96/100
1050/1050 [==============================] - 195s - loss: 0.8722 - mean_absolute_error: 0.8722 - mean_squared_error: 1.4535 - val_loss: 1.1982 - val_mean_absolute_error: 1.1982 - val_mean_squared_error: 2.2977
Epoch 97/100
1050/1050 [==============================] - 196s - loss: 0.8657 - mean_absolute_error: 0.8657 - mean_squared_error: 1.4628 - val_loss: 1.1698 - val_mean_absolute_error: 1.1698 - val_mean_squared_error: 2.2128
Epoch 98/100
1050/1050 [==============================] - 195s - loss: 0.8921 - mean_absolute_error: 0.8921 - mean_squared_error: 1.4980 - val_loss: 1.1453 - val_mean_absolute_error: 1.1453 - val_mean_squared_error: 2.2404
Epoch 99/100
1050/1050 [==============================] - 194s - loss: 0.8470 - mean_absolute_error: 0.8470 - mean_squared_error: 1.3477 - val_loss: 1.2506 - val_mean_absolute_error: 1.2506 - val_mean_squared_error: 2.6056
Epoch 100/100
1050/1050 [==============================] - 195s - loss: 0.8637 - mean_absolute_error: 0.8637 - mean_squared_error: 1.4454 - val_loss: 1.2089 - val_mean_absolute_error: 1.2089 - val_mean_squared_error: 2.4692

Parameters:
---------------------
h_STATES=25
EPOCHS=100
DENSES=10
Representation=fastText
EMBEDDING_DIM=50
MAX_SEQUENCE_LENGTH=50


____________________________________________________________________________________________________
attention_2 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_3 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_4 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
embedding_2 (Embedding)          (None, 50, 100)       424400                                       
____________________________________________________________________________________________________
attention_5 (Attention)          (None, 50, 40)        34300                                        
____________________________________________________________________________________________________
attention_6 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_7 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_8 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
maxoutdense_1 (MaxoutDense)      (None, 10)            3240        merge_1[0][0]                    
____________________________________________________________________________________________________
maxoutdense_2 (MaxoutDense)      (None, 1)             44          maxoutdense_1[0][0]              
====================================================================================================
Total params: 907,304
Trainable params: 111,004
Non-trainable params: 796,300
____________________________________________________________________________________________________
None
Happy learning!!!
Train on 1050 samples, validate on 450 samples
Epoch 1/20
1040/1050 [============================>.] - ETA: 23s - loss: 1.5288 - mean_absolute_error: 1.5288 - mean_squared_error: 3.2701Epoch 00000: val_loss improved from inf to 1.35953, saving model to /almac/ignacio/40_20_10_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2987s - loss: 1.5275 - mean_absolute_error: 1.5275 - mean_squared_error: 3.2651 - val_loss: 1.3595 - val_mean_absolute_error: 1.3595 - val_mean_squared_error: 2.3046
Epoch 2/20
1040/1050 [============================>.] - ETA: 25s - loss: 1.4085 - mean_absolute_error: 1.4085 - mean_squared_error: 2.5278Epoch 00001: val_loss improved from 1.35953 to 1.33055, saving model to /almac/ignacio/40_20_10_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 3125s - loss: 1.4059 - mean_absolute_error: 1.4059 - mean_squared_error: 2.5223 - val_loss: 1.3306 - val_mean_absolute_error: 1.3306 - val_mean_squared_error: 2.3116
Epoch 3/20
1040/1050 [============================>.] - ETA: 22s - loss: 1.3798 - mean_absolute_error: 1.3798 - mean_squared_error: 2.4443Epoch 00002: val_loss improved from 1.33055 to 1.22478, saving model to /almac/ignacio/40_20_10_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2671s - loss: 1.3797 - mean_absolute_error: 1.3797 - mean_squared_error: 2.4484 - val_loss: 1.2248 - val_mean_absolute_error: 1.2248 - val_mean_squared_error: 2.0110
Epoch 4/20
1040/1050 [============================>.] - ETA: 20s - loss: 1.2881 - mean_absolute_error: 1.2881 - mean_squared_error: 2.3816Epoch 00003: val_loss improved from 1.22478 to 1.21644, saving model to /almac/ignacio/40_20_10_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2488s - loss: 1.2871 - mean_absolute_error: 1.2871 - mean_squared_error: 2.3783 - val_loss: 1.2164 - val_mean_absolute_error: 1.2164 - val_mean_squared_error: 1.9845
Epoch 5/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.2618 - mean_absolute_error: 1.2618 - mean_squared_error: 2.2978Epoch 00004: val_loss did not improve
1050/1050 [==============================] - 2363s - loss: 1.2653 - mean_absolute_error: 1.2653 - mean_squared_error: 2.3057 - val_loss: 1.2259 - val_mean_absolute_error: 1.2259 - val_mean_squared_error: 2.1600
Epoch 6/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.2490 - mean_absolute_error: 1.2490 - mean_squared_error: 2.2563Epoch 00005: val_loss did not improve
1050/1050 [==============================] - 2464s - loss: 1.2461 - mean_absolute_error: 1.2461 - mean_squared_error: 2.2459 - val_loss: 1.2439 - val_mean_absolute_error: 1.2439 - val_mean_squared_error: 2.3019
Epoch 7/20
1040/1050 [============================>.] - ETA: 20s - loss: 1.2729 - mean_absolute_error: 1.2729 - mean_squared_error: 2.3058Epoch 00006: val_loss improved from 1.21644 to 1.20320, saving model to /almac/ignacio/40_20_10_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2532s - loss: 1.2746 - mean_absolute_error: 1.2746 - mean_squared_error: 2.3103 - val_loss: 1.2032 - val_mean_absolute_error: 1.2032 - val_mean_squared_error: 2.0193
Epoch 8/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.2202 - mean_absolute_error: 1.2202 - mean_squared_error: 2.1743Epoch 00007: val_loss improved from 1.20320 to 1.19266, saving model to /almac/ignacio/40_20_10_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2471s - loss: 1.2219 - mean_absolute_error: 1.2219 - mean_squared_error: 2.1796 - val_loss: 1.1927 - val_mean_absolute_error: 1.1927 - val_mean_squared_error: 1.9725
Epoch 9/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.2086 - mean_absolute_error: 1.2086 - mean_squared_error: 2.1562Epoch 00008: val_loss did not improve
1050/1050 [==============================] - 2222s - loss: 1.2084 - mean_absolute_error: 1.2084 - mean_squared_error: 2.1616 - val_loss: 1.2134 - val_mean_absolute_error: 1.2134 - val_mean_squared_error: 1.9882
Epoch 10/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.1903 - mean_absolute_error: 1.1903 - mean_squared_error: 2.1339Epoch 00009: val_loss did not improve
1050/1050 [==============================] - 2155s - loss: 1.1937 - mean_absolute_error: 1.1937 - mean_squared_error: 2.1434 - val_loss: 1.1948 - val_mean_absolute_error: 1.1948 - val_mean_squared_error: 2.1434
Epoch 11/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.1491 - mean_absolute_error: 1.1491 - mean_squared_error: 2.0395Epoch 00010: val_loss did not improve
1050/1050 [==============================] - 2049s - loss: 1.1560 - mean_absolute_error: 1.1560 - mean_squared_error: 2.0672 - val_loss: 1.2055 - val_mean_absolute_error: 1.2055 - val_mean_squared_error: 2.0887
Epoch 12/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.1738 - mean_absolute_error: 1.1738 - mean_squared_error: 2.1484Epoch 00011: val_loss did not improve
1050/1050 [==============================] - 2079s - loss: 1.1758 - mean_absolute_error: 1.1758 - mean_squared_error: 2.1567 - val_loss: 1.3005 - val_mean_absolute_error: 1.3005 - val_mean_squared_error: 2.4961
Epoch 13/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.1505 - mean_absolute_error: 1.1505 - mean_squared_error: 2.0505Epoch 00012: val_loss did not improve
1050/1050 [==============================] - 2023s - loss: 1.1459 - mean_absolute_error: 1.1459 - mean_squared_error: 2.0365 - val_loss: 1.2259 - val_mean_absolute_error: 1.2259 - val_mean_squared_error: 2.4653
Epoch 14/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.1320 - mean_absolute_error: 1.1320 - mean_squared_error: 2.0358Epoch 00013: val_loss improved from 1.19266 to 1.18684, saving model to /almac/ignacio/40_20_10_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 1991s - loss: 1.1345 - mean_absolute_error: 1.1345 - mean_squared_error: 2.0419 - val_loss: 1.1868 - val_mean_absolute_error: 1.1868 - val_mean_squared_error: 2.2083
Epoch 15/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.0990 - mean_absolute_error: 1.0990 - mean_squared_error: 1.9721Epoch 00014: val_loss did not improve
1050/1050 [==============================] - 2026s - loss: 1.0978 - mean_absolute_error: 1.0978 - mean_squared_error: 1.9698 - val_loss: 1.2235 - val_mean_absolute_error: 1.2235 - val_mean_squared_error: 2.1625
Epoch 16/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.1177 - mean_absolute_error: 1.1177 - mean_squared_error: 1.9532Epoch 00015: val_loss improved from 1.18684 to 1.17322, saving model to /almac/ignacio/40_20_10_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2013s - loss: 1.1187 - mean_absolute_error: 1.1187 - mean_squared_error: 1.9609 - val_loss: 1.1732 - val_mean_absolute_error: 1.1732 - val_mean_squared_error: 2.1218
Epoch 17/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.0938 - mean_absolute_error: 1.0938 - mean_squared_error: 1.9491Epoch 00016: val_loss did not improve
1050/1050 [==============================] - 2077s - loss: 1.0916 - mean_absolute_error: 1.0916 - mean_squared_error: 1.9473 - val_loss: 1.2452 - val_mean_absolute_error: 1.2452 - val_mean_squared_error: 2.3074
Epoch 18/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.0857 - mean_absolute_error: 1.0857 - mean_squared_error: 1.9023Epoch 00017: val_loss did not improve
1050/1050 [==============================] - 2224s - loss: 1.0868 - mean_absolute_error: 1.0868 - mean_squared_error: 1.9006 - val_loss: 1.1815 - val_mean_absolute_error: 1.1815 - val_mean_squared_error: 2.1743
Epoch 19/20
1040/1050 [============================>.] - ETA: 21s - loss: 1.0579 - mean_absolute_error: 1.0579 - mean_squared_error: 1.8432Epoch 00018: val_loss did not improve
1050/1050 [==============================] - 2672s - loss: 1.0558 - mean_absolute_error: 1.0558 - mean_squared_error: 1.8357 - val_loss: 1.1844 - val_mean_absolute_error: 1.1844 - val_mean_squared_error: 2.2164
Epoch 20/20
1040/1050 [============================>.] - ETA: 20s - loss: 1.0483 - mean_absolute_error: 1.0483 - mean_squared_error: 1.8225Epoch 00019: val_loss did not improve
1050/1050 [==============================] - 2497s - loss: 1.0459 - mean_absolute_error: 1.0459 - mean_squared_error: 1.8136 - val_loss: 1.3107 - val_mean_absolute_error: 1.3107 - val_mean_squared_error: 2.9719

Parameters:
---------------------
h_STATES=40
EPOCHS=20
DENSES=10
Representation=fastText
EMBEDDING_DIM=100
MAX_SEQUENCE_LENGTH=50
MODEL_TYPE=stacked

Using Theano backend.
ETA: 73590s 17left 4328.89avg  loLoanding train and valid dirs......
Starting training
Phrases in STS.input.headlines.txt 750 750
Phrases in STS.input.OnWN.txt 561 561
Phrases in STS.input.FNWN.txt 189 189
Total train phrases /almac/ignacio/data/sts_all/train-2013 1500
Total train phrases 3000
Starting training
Phrases in STS.input.track5.en-en.txt 250
Total train phrases /almac/ignacio/data/sts_all/valid-2017 250
Total train phrases 500
Spliting tab-separated files...
Labels shape:  (1500,)
Tokenizing files... [A]
Split training set into train and val... [A]
Tokenizing files... [B]
Split training set into train and val... [B]
Getting embedding matrix... from /almac/ignacio/data/fastText/wikiEn_Full_H100.model.vec
Found 4049540 word vectors.
Filling embedding matrices...
Compiling the model...
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
embedding_1 (Embedding)          (None, 50, 100)       371900                                       
____________________________________________________________________________________________________
attention_1 (Attention)          (None, 50, 40)        34300                                        
____________________________________________________________________________________________________
attention_2 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_3 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_4 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
embedding_2 (Embedding)          (None, 50, 100)       424400                                       
____________________________________________________________________________________________________
attention_5 (Attention)          (None, 50, 40)        34300                                        
____________________________________________________________________________________________________
attention_6 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_7 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_8 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
maxoutdense_1 (MaxoutDense)      (None, 30)            9720        merge_1[0][0]                    
____________________________________________________________________________________________________
maxoutdense_2 (MaxoutDense)      (None, 1)             124         maxoutdense_1[0][0]              
====================================================================================================
Total params: 913,864
Trainable params: 117,564
Non-trainable params: 796,300
____________________________________________________________________________________________________
None
Happy learning!!!
Train on 1050 samples, validate on 450 samples
Epoch 1/20
1040/1050 [============================>.] - ETA: 26s - loss: 1.4276 - mean_absolute_error: 1.4276 - mean_squared_error: 2.7220Epoch 00000: val_loss improved from inf to 1.38671, saving model to /almac/ignacio/40_20_30_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 3204s - loss: 1.4303 - mean_absolute_error: 1.4303 - mean_squared_error: 2.7386 - val_loss: 1.3867 - val_mean_absolute_error: 1.3867 - val_mean_squared_error: 2.7299
Epoch 2/20
1040/1050 [============================>.] - ETA: 24s - loss: 1.2859 - mean_absolute_error: 1.2859 - mean_squared_error: 2.3111Epoch 00001: val_loss improved from 1.38671 to 1.27424, saving model to /almac/ignacio/40_20_30_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 3009s - loss: 1.2858 - mean_absolute_error: 1.2858 - mean_squared_error: 2.3123 - val_loss: 1.2742 - val_mean_absolute_error: 1.2742 - val_mean_squared_error: 2.2106
Epoch 3/20
1040/1050 [============================>.] - ETA: 22s - loss: 1.2754 - mean_absolute_error: 1.2754 - mean_squared_error: 2.3545Epoch 00002: val_loss did not improve
1050/1050 [==============================] - 2831s - loss: 1.2767 - mean_absolute_error: 1.2767 - mean_squared_error: 2.3548 - val_loss: 1.3169 - val_mean_absolute_error: 1.3169 - val_mean_squared_error: 2.3129
Epoch 4/20
1040/1050 [============================>.] - ETA: 20s - loss: 1.1984 - mean_absolute_error: 1.1984 - mean_squared_error: 2.0914Epoch 00003: val_loss did not improve
1050/1050 [==============================] - 2595s - loss: 1.1945 - mean_absolute_error: 1.1945 - mean_squared_error: 2.0886 - val_loss: 1.4391 - val_mean_absolute_error: 1.4391 - val_mean_squared_error: 3.2157
Epoch 5/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.2092 - mean_absolute_error: 1.2092 - mean_squared_error: 2.1817Epoch 00004: val_loss did not improve
1050/1050 [==============================] - 2432s - loss: 1.2116 - mean_absolute_error: 1.2116 - mean_squared_error: 2.1856 - val_loss: 1.4263 - val_mean_absolute_error: 1.4263 - val_mean_squared_error: 3.1264
Epoch 6/20
1040/1050 [============================>.] - ETA: 20s - loss: 1.2313 - mean_absolute_error: 1.2313 - mean_squared_error: 2.2796Epoch 00005: val_loss did not improve
1050/1050 [==============================] - 2509s - loss: 1.2278 - mean_absolute_error: 1.2278 - mean_squared_error: 2.2688 - val_loss: 1.3084 - val_mean_absolute_error: 1.3084 - val_mean_squared_error: 2.6580
Epoch 7/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.1662 - mean_absolute_error: 1.1662 - mean_squared_error: 2.0922Epoch 00006: val_loss did not improve
1050/1050 [==============================] - 2281s - loss: 1.1646 - mean_absolute_error: 1.1646 - mean_squared_error: 2.0841 - val_loss: 1.3217 - val_mean_absolute_error: 1.3217 - val_mean_squared_error: 2.6216
Epoch 8/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.1944 - mean_absolute_error: 1.1944 - mean_squared_error: 2.1583Epoch 00007: val_loss did not improve
1050/1050 [==============================] - 2284s - loss: 1.1922 - mean_absolute_error: 1.1922 - mean_squared_error: 2.1512 - val_loss: 1.2863 - val_mean_absolute_error: 1.2863 - val_mean_squared_error: 2.3409
Epoch 9/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.1674 - mean_absolute_error: 1.1674 - mean_squared_error: 2.0438Epoch 00008: val_loss did not improve
1050/1050 [==============================] - 2287s - loss: 1.1691 - mean_absolute_error: 1.1691 - mean_squared_error: 2.0530 - val_loss: 1.2765 - val_mean_absolute_error: 1.2765 - val_mean_squared_error: 2.5193
Epoch 10/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.1176 - mean_absolute_error: 1.1176 - mean_squared_error: 1.9586Epoch 00009: val_loss did not improve
1050/1050 [==============================] - 2129s - loss: 1.1168 - mean_absolute_error: 1.1168 - mean_squared_error: 1.9558 - val_loss: 1.2851 - val_mean_absolute_error: 1.2851 - val_mean_squared_error: 2.6159
Epoch 11/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.0912 - mean_absolute_error: 1.0912 - mean_squared_error: 1.9345Epoch 00010: val_loss did not improve
1050/1050 [==============================] - 2050s - loss: 1.0857 - mean_absolute_error: 1.0857 - mean_squared_error: 1.9199 - val_loss: 1.5346 - val_mean_absolute_error: 1.5346 - val_mean_squared_error: 3.6908
Epoch 12/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.0859 - mean_absolute_error: 1.0859 - mean_squared_error: 1.9221Epoch 00011: val_loss improved from 1.27424 to 1.27093, saving model to /almac/ignacio/40_20_30_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2360s - loss: 1.0843 - mean_absolute_error: 1.0843 - mean_squared_error: 1.9178 - val_loss: 1.2709 - val_mean_absolute_error: 1.2709 - val_mean_squared_error: 2.5844
Epoch 13/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.0478 - mean_absolute_error: 1.0478 - mean_squared_error: 1.8461Epoch 00012: val_loss did not improve
1050/1050 [==============================] - 2107s - loss: 1.0453 - mean_absolute_error: 1.0453 - mean_squared_error: 1.8405 - val_loss: 1.4388 - val_mean_absolute_error: 1.4388 - val_mean_squared_error: 3.4505
Epoch 14/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.0335 - mean_absolute_error: 1.0335 - mean_squared_error: 1.7734Epoch 00013: val_loss did not improve
1050/1050 [==============================] - 2038s - loss: 1.0332 - mean_absolute_error: 1.0332 - mean_squared_error: 1.7755 - val_loss: 1.3024 - val_mean_absolute_error: 1.3024 - val_mean_squared_error: 2.6925
Epoch 15/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.0689 - mean_absolute_error: 1.0689 - mean_squared_error: 1.8783Epoch 00014: val_loss did not improve
1050/1050 [==============================] - 2035s - loss: 1.0681 - mean_absolute_error: 1.0681 - mean_squared_error: 1.8721 - val_loss: 1.2910 - val_mean_absolute_error: 1.2910 - val_mean_squared_error: 2.5669
Epoch 16/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.0550 - mean_absolute_error: 1.0550 - mean_squared_error: 1.9198Epoch 00015: val_loss improved from 1.27093 to 1.22732, saving model to /almac/ignacio/40_20_30_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2067s - loss: 1.0590 - mean_absolute_error: 1.0590 - mean_squared_error: 1.9265 - val_loss: 1.2273 - val_mean_absolute_error: 1.2273 - val_mean_squared_error: 2.2766
Epoch 17/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.0142 - mean_absolute_error: 1.0142 - mean_squared_error: 1.7247Epoch 00016: val_loss did not improve
1050/1050 [==============================] - 2042s - loss: 1.0134 - mean_absolute_error: 1.0134 - mean_squared_error: 1.7204 - val_loss: 1.2624 - val_mean_absolute_error: 1.2624 - val_mean_squared_error: 2.3938
Epoch 18/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.0224 - mean_absolute_error: 1.0224 - mean_squared_error: 1.7349Epoch 00017: val_loss did not improve
1050/1050 [==============================] - 2569s - loss: 1.0220 - mean_absolute_error: 1.0220 - mean_squared_error: 1.7337 - val_loss: 1.2640 - val_mean_absolute_error: 1.2640 - val_mean_squared_error: 2.3578
Epoch 19/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.0188 - mean_absolute_error: 1.0188 - mean_squared_error: 1.7546Epoch 00018: val_loss did not improve
1050/1050 [==============================] - 2394s - loss: 1.0180 - mean_absolute_error: 1.0180 - mean_squared_error: 1.7514 - val_loss: 1.3331 - val_mean_absolute_error: 1.3331 - val_mean_squared_error: 2.9346
Epoch 20/20
1040/1050 [============================>.] - ETA: 18s - loss: 0.9897 - mean_absolute_error: 0.9897 - mean_squared_error: 1.6941Epoch 00019: val_loss did not improve
1050/1050 [==============================] - 2328s - loss: 0.9866 - mean_absolute_error: 0.9866 - mean_squared_error: 1.6843 - val_loss: 1.2301 - val_mean_absolute_error: 1.2301 - val_mean_squared_error: 2.3584

Parameters:
---------------------
h_STATES=40
EPOCHS=20
DENSES=30
Representation=fastText
EMBEDDING_DIM=100
MAX_SEQUENCE_LENGTH=50
MODEL_TYPE=stacked

Using Theano backend.
ETA: 66033s 16left 4127.15avg  local:13/Loanding train and valid dirs......
Starting training
Phrases in STS.input.headlines.txt 750 750
Phrases in STS.input.OnWN.txt 561 561
Phrases in STS.input.FNWN.txt 189 189
Total train phrases /almac/ignacio/data/sts_all/train-2013 1500
Total train phrases 3000
Starting training
Phrases in STS.input.track5.en-en.txt 250
Total train phrases /almac/ignacio/data/sts_all/valid-2017 250
Total train phrases 500
Spliting tab-separated files...
Labels shape:  (1500,)
Tokenizing files... [A]
Split training set into train and val... [A]
Tokenizing files... [B]
Split training set into train and val... [B]
Getting embedding matrix... from /almac/ignacio/data/fastText/wikiEn_Full_H100.model.vec
Found 4049540 word vectors.
Filling embedding matrices...
Compiling the model...
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
embedding_1 (Embedding)          (None, 50, 100)       371900                                       
____________________________________________________________________________________________________
attention_1 (Attention)          (None, 50, 40)        34300                                        
____________________________________________________________________________________________________
attention_2 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_3 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_4 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
embedding_2 (Embedding)          (None, 50, 100)       424400                                       
____________________________________________________________________________________________________
attention_5 (Attention)          (None, 50, 40)        34300                                        
____________________________________________________________________________________________________
attention_6 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_7 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_8 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
maxoutdense_1 (MaxoutDense)      (None, 20)            6480        merge_1[0][0]                    
____________________________________________________________________________________________________
maxoutdense_2 (MaxoutDense)      (None, 1)             84          maxoutdense_1[0][0]              
====================================================================================================
Total params: 910,584
Trainable params: 114,284
Non-trainable params: 796,300
____________________________________________________________________________________________________
None
Happy learning!!!
Train on 1050 samples, validate on 450 samples
Epoch 1/20
1040/1050 [============================>.] - ETA: 24s - loss: 1.4141 - mean_absolute_error: 1.4141 - mean_squared_error: 2.6896Epoch 00000: val_loss improved from inf to 1.41768, saving model to /almac/ignacio/40_20_20_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 3102s - loss: 1.4116 - mean_absolute_error: 1.4116 - mean_squared_error: 2.6809 - val_loss: 1.4177 - val_mean_absolute_error: 1.4177 - val_mean_squared_error: 2.7852
Epoch 2/20
1040/1050 [============================>.] - ETA: 25s - loss: 1.3874 - mean_absolute_error: 1.3874 - mean_squared_error: 2.5382Epoch 00001: val_loss improved from 1.41768 to 1.36109, saving model to /almac/ignacio/40_20_20_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 3132s - loss: 1.3885 - mean_absolute_error: 1.3885 - mean_squared_error: 2.5427 - val_loss: 1.3611 - val_mean_absolute_error: 1.3611 - val_mean_squared_error: 2.3624
Epoch 3/20
1040/1050 [============================>.] - ETA: 22s - loss: 1.3529 - mean_absolute_error: 1.3529 - mean_squared_error: 2.4574Epoch 00002: val_loss improved from 1.36109 to 1.29562, saving model to /almac/ignacio/40_20_20_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2715s - loss: 1.3547 - mean_absolute_error: 1.3547 - mean_squared_error: 2.4632 - val_loss: 1.2956 - val_mean_absolute_error: 1.2956 - val_mean_squared_error: 2.3608
Epoch 4/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.3236 - mean_absolute_error: 1.3236 - mean_squared_error: 2.4377Epoch 00003: val_loss did not improve
1050/1050 [==============================] - 2480s - loss: 1.3269 - mean_absolute_error: 1.3269 - mean_squared_error: 2.4490 - val_loss: 1.5780 - val_mean_absolute_error: 1.5780 - val_mean_squared_error: 3.6315
Epoch 5/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.2832 - mean_absolute_error: 1.2832 - mean_squared_error: 2.3384Epoch 00004: val_loss improved from 1.29562 to 1.25044, saving model to /almac/ignacio/40_20_20_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2354s - loss: 1.2818 - mean_absolute_error: 1.2818 - mean_squared_error: 2.3343 - val_loss: 1.2504 - val_mean_absolute_error: 1.2504 - val_mean_squared_error: 2.1539
Epoch 6/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.2591 - mean_absolute_error: 1.2591 - mean_squared_error: 2.2560Epoch 00005: val_loss improved from 1.25044 to 1.24010, saving model to /almac/ignacio/40_20_20_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2115s - loss: 1.2595 - mean_absolute_error: 1.2595 - mean_squared_error: 2.2605 - val_loss: 1.2401 - val_mean_absolute_error: 1.2401 - val_mean_squared_error: 2.3934
Epoch 7/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.2868 - mean_absolute_error: 1.2868 - mean_squared_error: 2.3243Epoch 00006: val_loss did not improve
1050/1050 [==============================] - 2396s - loss: 1.2888 - mean_absolute_error: 1.2888 - mean_squared_error: 2.3277 - val_loss: 1.2693 - val_mean_absolute_error: 1.2693 - val_mean_squared_error: 2.1544
Epoch 8/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.2501 - mean_absolute_error: 1.2501 - mean_squared_error: 2.3057Epoch 00007: val_loss did not improve
1050/1050 [==============================] - 2295s - loss: 1.2497 - mean_absolute_error: 1.2497 - mean_squared_error: 2.3042 - val_loss: 1.3217 - val_mean_absolute_error: 1.3217 - val_mean_squared_error: 2.3454
Epoch 9/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.2622 - mean_absolute_error: 1.2622 - mean_squared_error: 2.2621Epoch 00008: val_loss did not improve
1050/1050 [==============================] - 2020s - loss: 1.2612 - mean_absolute_error: 1.2612 - mean_squared_error: 2.2663 - val_loss: 1.2607 - val_mean_absolute_error: 1.2607 - val_mean_squared_error: 2.1491
Epoch 10/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.2552 - mean_absolute_error: 1.2552 - mean_squared_error: 2.3137Epoch 00009: val_loss did not improve
1050/1050 [==============================] - 2144s - loss: 1.2567 - mean_absolute_error: 1.2567 - mean_squared_error: 2.3149 - val_loss: 1.2424 - val_mean_absolute_error: 1.2424 - val_mean_squared_error: 2.0541
Epoch 11/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.2014 - mean_absolute_error: 1.2014 - mean_squared_error: 2.0808Epoch 00010: val_loss did not improve
1050/1050 [==============================] - 2108s - loss: 1.2020 - mean_absolute_error: 1.2020 - mean_squared_error: 2.0840 - val_loss: 1.2407 - val_mean_absolute_error: 1.2407 - val_mean_squared_error: 2.6099
Epoch 12/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.2005 - mean_absolute_error: 1.2005 - mean_squared_error: 2.1703Epoch 00011: val_loss improved from 1.24010 to 1.20269, saving model to /almac/ignacio/40_20_20_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2133s - loss: 1.2012 - mean_absolute_error: 1.2012 - mean_squared_error: 2.1740 - val_loss: 1.2027 - val_mean_absolute_error: 1.2027 - val_mean_squared_error: 2.3068
Epoch 13/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.1769 - mean_absolute_error: 1.1769 - mean_squared_error: 2.1038Epoch 00012: val_loss improved from 1.20269 to 1.17699, saving model to /almac/ignacio/40_20_20_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2359s - loss: 1.1755 - mean_absolute_error: 1.1755 - mean_squared_error: 2.0987 - val_loss: 1.1770 - val_mean_absolute_error: 1.1770 - val_mean_squared_error: 2.1604
Epoch 14/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.1482 - mean_absolute_error: 1.1482 - mean_squared_error: 2.0041Epoch 00013: val_loss did not improve
1050/1050 [==============================] - 2334s - loss: 1.1474 - mean_absolute_error: 1.1474 - mean_squared_error: 2.0023 - val_loss: 1.1950 - val_mean_absolute_error: 1.1950 - val_mean_squared_error: 2.1474
Epoch 15/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.1402 - mean_absolute_error: 1.1402 - mean_squared_error: 2.0304Epoch 00014: val_loss did not improve
1050/1050 [==============================] - 2253s - loss: 1.1377 - mean_absolute_error: 1.1377 - mean_squared_error: 2.0221 - val_loss: 1.3151 - val_mean_absolute_error: 1.3151 - val_mean_squared_error: 2.4105
Epoch 16/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.1541 - mean_absolute_error: 1.1541 - mean_squared_error: 2.0336Epoch 00015: val_loss improved from 1.17699 to 1.15476, saving model to /almac/ignacio/40_20_20_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2200s - loss: 1.1546 - mean_absolute_error: 1.1546 - mean_squared_error: 2.0341 - val_loss: 1.1548 - val_mean_absolute_error: 1.1548 - val_mean_squared_error: 1.9597
Epoch 17/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.1025 - mean_absolute_error: 1.1025 - mean_squared_error: 1.9134Epoch 00016: val_loss did not improve
1050/1050 [==============================] - 2138s - loss: 1.1063 - mean_absolute_error: 1.1063 - mean_squared_error: 1.9235 - val_loss: 1.1616 - val_mean_absolute_error: 1.1616 - val_mean_squared_error: 2.0312
Epoch 18/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.0898 - mean_absolute_error: 1.0898 - mean_squared_error: 1.8759Epoch 00017: val_loss did not improve
1050/1050 [==============================] - 2479s - loss: 1.0887 - mean_absolute_error: 1.0887 - mean_squared_error: 1.8703 - val_loss: 1.2052 - val_mean_absolute_error: 1.2052 - val_mean_squared_error: 2.1450
Epoch 19/20
1040/1050 [============================>.] - ETA: 22s - loss: 1.0811 - mean_absolute_error: 1.0811 - mean_squared_error: 1.8774Epoch 00018: val_loss did not improve
1050/1050 [==============================] - 2742s - loss: 1.0762 - mean_absolute_error: 1.0762 - mean_squared_error: 1.8638 - val_loss: 1.1684 - val_mean_absolute_error: 1.1684 - val_mean_squared_error: 2.0857
Epoch 20/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.0685 - mean_absolute_error: 1.0685 - mean_squared_error: 1.8519Epoch 00019: val_loss did not improve
1050/1050 [==============================] - 2502s - loss: 1.0697 - mean_absolute_error: 1.0697 - mean_squared_error: 1.8562 - val_loss: 1.1940 - val_mean_absolute_error: 1.1940 - val_mean_squared_error: 2.2198

Parameters:
---------------------
h_STATES=40
EPOCHS=20
DENSES=20
Representation=fastText
EMBEDDING_DIM=100
MAX_SEQUENCE_LENGTH=50
MODEL_TYPE=stacked

Using Theano backend.
ETA: 66054s 16left 4128.45avg  local:13/20/100%/4128.5s Loanding train and valid dirs......
Starting training
Phrases in STS.input.headlines.txt 750 750
Phrases in STS.input.OnWN.txt 561 561
Phrases in STS.input.FNWN.txt 189 189
Total train phrases /almac/ignacio/data/sts_all/train-2013 1500
Total train phrases 3000
Starting training
Phrases in STS.input.track5.en-en.txt 250
Total train phrases /almac/ignacio/data/sts_all/valid-2017 250
Total train phrases 500
Spliting tab-separated files...
Labels shape:  (1500,)
Tokenizing files... [A]
Split training set into train and val... [A]
Tokenizing files... [B]
Split training set into train and val... [B]
Getting embedding matrix... from /almac/ignacio/data/fastText/wikiEn_Full_H100.model.vec
Found 4049540 word vectors.
Filling embedding matrices...
Compiling the model...
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
embedding_1 (Embedding)          (None, 50, 100)       371900                                       
____________________________________________________________________________________________________
attention_1 (Attention)          (None, 50, 40)        34300                                        
____________________________________________________________________________________________________
attention_2 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_3 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_4 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
embedding_2 (Embedding)          (None, 50, 100)       424400                                       
____________________________________________________________________________________________________
attention_5 (Attention)          (None, 50, 40)        34300                                        
____________________________________________________________________________________________________
attention_6 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_7 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_8 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
maxoutdense_1 (MaxoutDense)      (None, 100)           32400       merge_1[0][0]                    
____________________________________________________________________________________________________
maxoutdense_2 (MaxoutDense)      (None, 1)             404         maxoutdense_1[0][0]              
====================================================================================================
Total params: 936,824
Trainable params: 140,524
Non-trainable params: 796,300
____________________________________________________________________________________________________
None
Happy learning!!!
Train on 1050 samples, validate on 450 samples
Epoch 1/20
1040/1050 [============================>.] - ETA: 27s - loss: 1.4148 - mean_absolute_error: 1.4148 - mean_squared_error: 2.7513Epoch 00000: val_loss improved from inf to 1.53630, saving model to /almac/ignacio/40_20_100_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 3274s - loss: 1.4189 - mean_absolute_error: 1.4189 - mean_squared_error: 2.7617 - val_loss: 1.5363 - val_mean_absolute_error: 1.5363 - val_mean_squared_error: 3.2968
Epoch 2/20
1040/1050 [============================>.] - ETA: 25s - loss: 1.4070 - mean_absolute_error: 1.4070 - mean_squared_error: 2.6304Epoch 00001: val_loss did not improve
1050/1050 [==============================] - 3105s - loss: 1.4041 - mean_absolute_error: 1.4041 - mean_squared_error: 2.6208 - val_loss: 1.8275 - val_mean_absolute_error: 1.8275 - val_mean_squared_error: 5.3500
Epoch 3/20
1040/1050 [============================>.] - ETA: 20s - loss: 1.3925 - mean_absolute_error: 1.3925 - mean_squared_error: 2.5772Epoch 00002: val_loss improved from 1.53630 to 1.39060, saving model to /almac/ignacio/40_20_100_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2517s - loss: 1.3953 - mean_absolute_error: 1.3953 - mean_squared_error: 2.5891 - val_loss: 1.3906 - val_mean_absolute_error: 1.3906 - val_mean_squared_error: 2.5093
Epoch 4/20
1040/1050 [============================>.] - ETA: 20s - loss: 1.4015 - mean_absolute_error: 1.4015 - mean_squared_error: 2.7405Epoch 00003: val_loss improved from 1.39060 to 1.23028, saving model to /almac/ignacio/40_20_100_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2517s - loss: 1.4029 - mean_absolute_error: 1.4029 - mean_squared_error: 2.7406 - val_loss: 1.2303 - val_mean_absolute_error: 1.2303 - val_mean_squared_error: 2.0152
Epoch 5/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.3202 - mean_absolute_error: 1.3202 - mean_squared_error: 2.4278Epoch 00004: val_loss did not improve
1050/1050 [==============================] - 2116s - loss: 1.3226 - mean_absolute_error: 1.3226 - mean_squared_error: 2.4363 - val_loss: 1.6226 - val_mean_absolute_error: 1.6226 - val_mean_squared_error: 4.0435
Epoch 6/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.3128 - mean_absolute_error: 1.3128 - mean_squared_error: 2.4397Epoch 00005: val_loss improved from 1.23028 to 1.15330, saving model to /almac/ignacio/40_20_100_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2115s - loss: 1.3185 - mean_absolute_error: 1.3185 - mean_squared_error: 2.4598 - val_loss: 1.1533 - val_mean_absolute_error: 1.1533 - val_mean_squared_error: 2.1238
Epoch 7/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.3244 - mean_absolute_error: 1.3244 - mean_squared_error: 2.5245Epoch 00006: val_loss did not improve
1050/1050 [==============================] - 2170s - loss: 1.3246 - mean_absolute_error: 1.3246 - mean_squared_error: 2.5228 - val_loss: 1.1703 - val_mean_absolute_error: 1.1703 - val_mean_squared_error: 1.9015
Epoch 8/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.2327 - mean_absolute_error: 1.2327 - mean_squared_error: 2.2513Epoch 00007: val_loss did not improve
1050/1050 [==============================] - 2213s - loss: 1.2337 - mean_absolute_error: 1.2337 - mean_squared_error: 2.2536 - val_loss: 1.1789 - val_mean_absolute_error: 1.1789 - val_mean_squared_error: 1.9624
Epoch 9/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.2162 - mean_absolute_error: 1.2162 - mean_squared_error: 2.2583Epoch 00008: val_loss did not improve
1050/1050 [==============================] - 2163s - loss: 1.2131 - mean_absolute_error: 1.2131 - mean_squared_error: 2.2464 - val_loss: 1.2595 - val_mean_absolute_error: 1.2595 - val_mean_squared_error: 2.7792
Epoch 10/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.2011 - mean_absolute_error: 1.2011 - mean_squared_error: 2.2060Epoch 00009: val_loss improved from 1.15330 to 1.14014, saving model to /almac/ignacio/40_20_100_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2204s - loss: 1.2013 - mean_absolute_error: 1.2013 - mean_squared_error: 2.2050 - val_loss: 1.1401 - val_mean_absolute_error: 1.1401 - val_mean_squared_error: 1.9820
Epoch 11/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.1964 - mean_absolute_error: 1.1964 - mean_squared_error: 2.1860Epoch 00010: val_loss did not improve
1050/1050 [==============================] - 2116s - loss: 1.2000 - mean_absolute_error: 1.2000 - mean_squared_error: 2.1947 - val_loss: 1.4270 - val_mean_absolute_error: 1.4270 - val_mean_squared_error: 3.1032
Epoch 12/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.2214 - mean_absolute_error: 1.2214 - mean_squared_error: 2.2509Epoch 00011: val_loss did not improve
1050/1050 [==============================] - 2084s - loss: 1.2223 - mean_absolute_error: 1.2223 - mean_squared_error: 2.2486 - val_loss: 1.1837 - val_mean_absolute_error: 1.1837 - val_mean_squared_error: 2.0492
Epoch 13/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.1900 - mean_absolute_error: 1.1900 - mean_squared_error: 2.1631Epoch 00012: val_loss improved from 1.14014 to 1.13876, saving model to /almac/ignacio/40_20_100_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2129s - loss: 1.1879 - mean_absolute_error: 1.1879 - mean_squared_error: 2.1542 - val_loss: 1.1388 - val_mean_absolute_error: 1.1388 - val_mean_squared_error: 2.2025
Epoch 14/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.1700 - mean_absolute_error: 1.1700 - mean_squared_error: 2.1489Epoch 00013: val_loss improved from 1.13876 to 1.12950, saving model to /almac/ignacio/40_20_100_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2165s - loss: 1.1704 - mean_absolute_error: 1.1704 - mean_squared_error: 2.1507 - val_loss: 1.1295 - val_mean_absolute_error: 1.1295 - val_mean_squared_error: 2.0737
Epoch 15/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.1820 - mean_absolute_error: 1.1820 - mean_squared_error: 2.1993Epoch 00014: val_loss did not improve
1050/1050 [==============================] - 2053s - loss: 1.1850 - mean_absolute_error: 1.1850 - mean_squared_error: 2.2114 - val_loss: 1.2283 - val_mean_absolute_error: 1.2283 - val_mean_squared_error: 2.6176
Epoch 16/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.1804 - mean_absolute_error: 1.1804 - mean_squared_error: 2.1458Epoch 00015: val_loss did not improve
1050/1050 [==============================] - 2103s - loss: 1.1808 - mean_absolute_error: 1.1808 - mean_squared_error: 2.1454 - val_loss: 1.2293 - val_mean_absolute_error: 1.2293 - val_mean_squared_error: 2.0644
Epoch 17/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.1243 - mean_absolute_error: 1.1243 - mean_squared_error: 1.9941Epoch 00016: val_loss improved from 1.12950 to 1.12493, saving model to /almac/ignacio/40_20_100_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2038s - loss: 1.1276 - mean_absolute_error: 1.1276 - mean_squared_error: 2.0036 - val_loss: 1.1249 - val_mean_absolute_error: 1.1249 - val_mean_squared_error: 1.9555
Epoch 18/20
1040/1050 [============================>.] - ETA: 21s - loss: 1.1186 - mean_absolute_error: 1.1186 - mean_squared_error: 1.9664Epoch 00017: val_loss did not improve
1050/1050 [==============================] - 2662s - loss: 1.1188 - mean_absolute_error: 1.1188 - mean_squared_error: 1.9689 - val_loss: 1.1460 - val_mean_absolute_error: 1.1460 - val_mean_squared_error: 1.9385
Epoch 19/20
1040/1050 [============================>.] - ETA: 20s - loss: 1.1125 - mean_absolute_error: 1.1125 - mean_squared_error: 2.0135Epoch 00018: val_loss did not improve
1050/1050 [==============================] - 2498s - loss: 1.1147 - mean_absolute_error: 1.1147 - mean_squared_error: 2.0175 - val_loss: 1.1277 - val_mean_absolute_error: 1.1277 - val_mean_squared_error: 2.1140
Epoch 20/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.0980 - mean_absolute_error: 1.0980 - mean_squared_error: 1.9663Epoch 00019: val_loss did not improve
1050/1050 [==============================] - 2329s - loss: 1.0985 - mean_absolute_error: 1.0985 - mean_squared_error: 1.9642 - val_loss: 1.1545 - val_mean_absolute_error: 1.1545 - val_mean_squared_error: 1.9556

Parameters:
---------------------
h_STATES=40
EPOCHS=20
DENSES=100
Representation=fastText
EMBEDDING_DIM=100
MAX_SEQUENCE_LENGTH=50
MODEL_TYPE=stacked

Using Theano backend.
ETA: 52998s 14left 3785.64avg  local:13/22/100%/3Loanding train and valid dirs......
Starting training
Phrases in STS.input.headlines.txt 750 750
Phrases in STS.input.OnWN.txt 561 561
Phrases in STS.input.FNWN.txt 189 189
Total train phrases /almac/ignacio/data/sts_all/train-2013 1500
Total train phrases 3000
Starting training
Phrases in STS.input.track5.en-en.txt 250
Total train phrases /almac/ignacio/data/sts_all/valid-2017 250
Total train phrases 500
Spliting tab-separated files...
Labels shape:  (1500,)
Tokenizing files... [A]
Split training set into train and val... [A]
Tokenizing files... [B]
Split training set into train and val... [B]
Getting embedding matrix... from /almac/ignacio/data/fastText/wikiEn_Full_H100.model.vec
Found 4049540 word vectors.
Filling embedding matrices...
Compiling the model...
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
embedding_1 (Embedding)          (None, 50, 100)       371900                                       
____________________________________________________________________________________________________
attention_1 (Attention)          (None, 50, 40)        34300                                        
____________________________________________________________________________________________________
attention_2 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_3 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_4 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
embedding_2 (Embedding)          (None, 50, 100)       424400                                       
____________________________________________________________________________________________________
attention_5 (Attention)          (None, 50, 40)        34300                                        
____________________________________________________________________________________________________
attention_6 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_7 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_8 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
maxoutdense_1 (MaxoutDense)      (None, 40)            12960       merge_1[0][0]                    
____________________________________________________________________________________________________
maxoutdense_2 (MaxoutDense)      (None, 1)             164         maxoutdense_1[0][0]              
====================================================================================================
Total params: 917,144
Trainable params: 120,844
Non-trainable params: 796,300
____________________________________________________________________________________________________
None
Happy learning!!!
Train on 1050 samples, validate on 450 samples
Epoch 1/20
1040/1050 [============================>.] - ETA: 26s - loss: 1.4513 - mean_absolute_error: 1.4513 - mean_squared_error: 2.8215Epoch 00000: val_loss improved from inf to 1.36795, saving model to /almac/ignacio/40_20_40_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 3240s - loss: 1.4518 - mean_absolute_error: 1.4518 - mean_squared_error: 2.8196 - val_loss: 1.3680 - val_mean_absolute_error: 1.3680 - val_mean_squared_error: 2.3490
Epoch 2/20
1040/1050 [============================>.] - ETA: 25s - loss: 1.3994 - mean_absolute_error: 1.3994 - mean_squared_error: 2.6043Epoch 00001: val_loss did not improve
1050/1050 [==============================] - 3157s - loss: 1.3972 - mean_absolute_error: 1.3972 - mean_squared_error: 2.5987 - val_loss: 1.4922 - val_mean_absolute_error: 1.4922 - val_mean_squared_error: 3.2065
Epoch 3/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.3915 - mean_absolute_error: 1.3915 - mean_squared_error: 2.5874Epoch 00002: val_loss did not improve
1050/1050 [==============================] - 2427s - loss: 1.3946 - mean_absolute_error: 1.3946 - mean_squared_error: 2.5959 - val_loss: 1.3703 - val_mean_absolute_error: 1.3703 - val_mean_squared_error: 2.3523
Epoch 4/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.3939 - mean_absolute_error: 1.3939 - mean_squared_error: 2.5727Epoch 00003: val_loss improved from 1.36795 to 1.28527, saving model to /almac/ignacio/40_20_40_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2512s - loss: 1.3919 - mean_absolute_error: 1.3919 - mean_squared_error: 2.5716 - val_loss: 1.2853 - val_mean_absolute_error: 1.2853 - val_mean_squared_error: 2.1183
Epoch 5/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.3020 - mean_absolute_error: 1.3020 - mean_squared_error: 2.3694Epoch 00004: val_loss improved from 1.28527 to 1.21355, saving model to /almac/ignacio/40_20_40_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2275s - loss: 1.3024 - mean_absolute_error: 1.3024 - mean_squared_error: 2.3739 - val_loss: 1.2136 - val_mean_absolute_error: 1.2136 - val_mean_squared_error: 2.1824
Epoch 6/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.3009 - mean_absolute_error: 1.3009 - mean_squared_error: 2.4436Epoch 00005: val_loss did not improve
1050/1050 [==============================] - 2122s - loss: 1.2985 - mean_absolute_error: 1.2985 - mean_squared_error: 2.4354 - val_loss: 1.2352 - val_mean_absolute_error: 1.2352 - val_mean_squared_error: 2.0246
Epoch 7/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.2712 - mean_absolute_error: 1.2712 - mean_squared_error: 2.2954Epoch 00006: val_loss improved from 1.21355 to 1.20963, saving model to /almac/ignacio/40_20_40_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2271s - loss: 1.2720 - mean_absolute_error: 1.2720 - mean_squared_error: 2.2985 - val_loss: 1.2096 - val_mean_absolute_error: 1.2096 - val_mean_squared_error: 1.9839
Epoch 8/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.2509 - mean_absolute_error: 1.2509 - mean_squared_error: 2.2247Epoch 00007: val_loss improved from 1.20963 to 1.20380, saving model to /almac/ignacio/40_20_40_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2430s - loss: 1.2480 - mean_absolute_error: 1.2480 - mean_squared_error: 2.2168 - val_loss: 1.2038 - val_mean_absolute_error: 1.2038 - val_mean_squared_error: 2.4203
Epoch 9/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.2350 - mean_absolute_error: 1.2350 - mean_squared_error: 2.2455Epoch 00008: val_loss did not improve
1050/1050 [==============================] - 2112s - loss: 1.2348 - mean_absolute_error: 1.2348 - mean_squared_error: 2.2433 - val_loss: 1.2270 - val_mean_absolute_error: 1.2270 - val_mean_squared_error: 2.0797
Epoch 10/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.2276 - mean_absolute_error: 1.2276 - mean_squared_error: 2.2485Epoch 00009: val_loss improved from 1.20380 to 1.15868, saving model to /almac/ignacio/40_20_40_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2150s - loss: 1.2313 - mean_absolute_error: 1.2313 - mean_squared_error: 2.2572 - val_loss: 1.1587 - val_mean_absolute_error: 1.1587 - val_mean_squared_error: 1.9536
Epoch 11/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.2217 - mean_absolute_error: 1.2217 - mean_squared_error: 2.1725Epoch 00010: val_loss did not improve
1050/1050 [==============================] - 2240s - loss: 1.2256 - mean_absolute_error: 1.2256 - mean_squared_error: 2.1942 - val_loss: 1.1896 - val_mean_absolute_error: 1.1896 - val_mean_squared_error: 2.1069
Epoch 12/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.1929 - mean_absolute_error: 1.1929 - mean_squared_error: 2.0915Epoch 00011: val_loss did not improve
1050/1050 [==============================] - 2155s - loss: 1.1887 - mean_absolute_error: 1.1887 - mean_squared_error: 2.0788 - val_loss: 1.1617 - val_mean_absolute_error: 1.1617 - val_mean_squared_error: 1.9207
Epoch 13/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.1457 - mean_absolute_error: 1.1457 - mean_squared_error: 2.0292Epoch 00012: val_loss did not improve
1050/1050 [==============================] - 2263s - loss: 1.1493 - mean_absolute_error: 1.1493 - mean_squared_error: 2.0383 - val_loss: 1.1760 - val_mean_absolute_error: 1.1760 - val_mean_squared_error: 2.0623
Epoch 14/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.1394 - mean_absolute_error: 1.1394 - mean_squared_error: 1.9871Epoch 00013: val_loss improved from 1.15868 to 1.13617, saving model to /almac/ignacio/40_20_40_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2014s - loss: 1.1443 - mean_absolute_error: 1.1443 - mean_squared_error: 1.9999 - val_loss: 1.1362 - val_mean_absolute_error: 1.1362 - val_mean_squared_error: 1.9920
Epoch 15/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.1421 - mean_absolute_error: 1.1421 - mean_squared_error: 2.0221Epoch 00014: val_loss did not improve
1050/1050 [==============================] - 2153s - loss: 1.1387 - mean_absolute_error: 1.1387 - mean_squared_error: 2.0117 - val_loss: 1.1423 - val_mean_absolute_error: 1.1423 - val_mean_squared_error: 1.9384
Epoch 16/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.1010 - mean_absolute_error: 1.1010 - mean_squared_error: 1.9121Epoch 00015: val_loss did not improve
1050/1050 [==============================] - 2180s - loss: 1.1002 - mean_absolute_error: 1.1002 - mean_squared_error: 1.9079 - val_loss: 1.1380 - val_mean_absolute_error: 1.1380 - val_mean_squared_error: 2.0995
Epoch 17/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.0599 - mean_absolute_error: 1.0599 - mean_squared_error: 1.8823Epoch 00016: val_loss did not improve
1050/1050 [==============================] - 2249s - loss: 1.0609 - mean_absolute_error: 1.0609 - mean_squared_error: 1.8799 - val_loss: 1.1588 - val_mean_absolute_error: 1.1588 - val_mean_squared_error: 2.1230
Epoch 18/20
1040/1050 [============================>.] - ETA: 21s - loss: 1.0620 - mean_absolute_error: 1.0620 - mean_squared_error: 1.8072Epoch 00017: val_loss did not improve
1050/1050 [==============================] - 2689s - loss: 1.0669 - mean_absolute_error: 1.0669 - mean_squared_error: 1.8254 - val_loss: 1.1700 - val_mean_absolute_error: 1.1700 - val_mean_squared_error: 2.1382
Epoch 19/20
1040/1050 [============================>.] - ETA: 21s - loss: 1.0371 - mean_absolute_error: 1.0371 - mean_squared_error: 1.8077Epoch 00018: val_loss did not improve
1050/1050 [==============================] - 2607s - loss: 1.0350 - mean_absolute_error: 1.0350 - mean_squared_error: 1.8024 - val_loss: 1.1645 - val_mean_absolute_error: 1.1645 - val_mean_squared_error: 2.1764
Epoch 20/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.0524 - mean_absolute_error: 1.0524 - mean_squared_error: 1.8869Epoch 00019: val_loss did not improve
1050/1050 [==============================] - 2473s - loss: 1.0522 - mean_absolute_error: 1.0522 - mean_squared_error: 1.8845 - val_loss: 1.1439 - val_mean_absolute_error: 1.1439 - val_mean_squared_error: 1.9956

Parameters:
---------------------
h_STATES=40
EPOCHS=20
DENSES=40
Representation=fastText
EMBEDDING_DIM=100
MAX_SEQUENCE_LENGTH=50
MODEL_TYPE=stacked

Using Theano backend.
ETA: 47307s 13left 3639.Loanding train and valid dirs......
Starting training
Phrases in STS.input.headlines.txt 750 750
Phrases in STS.input.OnWN.txt 561 561
Phrases in STS.input.FNWN.txt 189 189
Total train phrases /almac/ignacio/data/sts_all/train-2013 1500
Total train phrases 3000
Starting training
Phrases in STS.input.track5.en-en.txt 250
Total train phrases /almac/ignacio/data/sts_all/valid-2017 250
Total train phrases 500
Spliting tab-separated files...
Labels shape:  (1500,)
Tokenizing files... [A]
Split training set into train and val... [A]
Tokenizing files... [B]
Split training set into train and val... [B]
Getting embedding matrix... from /almac/ignacio/data/fastText/wikiEn_Full_H100.model.vec
Found 4049540 word vectors.
Filling embedding matrices...
Compiling the model...
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
embedding_1 (Embedding)          (None, 50, 100)       371900                                       
____________________________________________________________________________________________________
attention_1 (Attention)          (None, 50, 40)        34300                                        
____________________________________________________________________________________________________
attention_2 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_3 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_4 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
embedding_2 (Embedding)          (None, 50, 100)       424400                                       
____________________________________________________________________________________________________
attention_5 (Attention)          (None, 50, 40)        34300                                        
____________________________________________________________________________________________________
attention_6 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_7 (Attention)          (None, 50, 40)        6520                                         
____________________________________________________________________________________________________
attention_8 (Attention)          (None, 40)            6520                                         
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 40)            0                                            
____________________________________________________________________________________________________
maxoutdense_1 (MaxoutDense)      (None, 50)            16200       merge_1[0][0]                    
____________________________________________________________________________________________________
maxoutdense_2 (MaxoutDense)      (None, 1)             204         maxoutdense_1[0][0]              
====================================================================================================
Total params: 920,424
Trainable params: 124,124
Non-trainable params: 796,300
____________________________________________________________________________________________________
None
Happy learning!!!
Train on 1050 samples, validate on 450 samples
Epoch 1/20
1040/1050 [============================>.] - ETA: 27s - loss: 1.4522 - mean_absolute_error: 1.4522 - mean_squared_error: 2.8551Epoch 00000: val_loss improved from inf to 1.39667, saving model to /almac/ignacio/40_20_50_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 3276s - loss: 1.4485 - mean_absolute_error: 1.4485 - mean_squared_error: 2.8490 - val_loss: 1.3967 - val_mean_absolute_error: 1.3967 - val_mean_squared_error: 2.5961
Epoch 2/20
1040/1050 [============================>.] - ETA: 25s - loss: 1.4202 - mean_absolute_error: 1.4202 - mean_squared_error: 2.6933Epoch 00001: val_loss improved from 1.39667 to 1.38892, saving model to /almac/ignacio/40_20_50_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 3078s - loss: 1.4183 - mean_absolute_error: 1.4183 - mean_squared_error: 2.6874 - val_loss: 1.3889 - val_mean_absolute_error: 1.3889 - val_mean_squared_error: 2.4995
Epoch 3/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.4064 - mean_absolute_error: 1.4064 - mean_squared_error: 2.5818Epoch 00002: val_loss improved from 1.38892 to 1.33875, saving model to /almac/ignacio/40_20_50_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2411s - loss: 1.4089 - mean_absolute_error: 1.4089 - mean_squared_error: 2.5923 - val_loss: 1.3387 - val_mean_absolute_error: 1.3387 - val_mean_squared_error: 2.2651
Epoch 4/20
1040/1050 [============================>.] - ETA: 20s - loss: 1.3461 - mean_absolute_error: 1.3461 - mean_squared_error: 2.4726Epoch 00003: val_loss improved from 1.33875 to 1.33298, saving model to /almac/ignacio/40_20_50_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2587s - loss: 1.3474 - mean_absolute_error: 1.3474 - mean_squared_error: 2.4748 - val_loss: 1.3330 - val_mean_absolute_error: 1.3330 - val_mean_squared_error: 2.3610
Epoch 5/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.3181 - mean_absolute_error: 1.3181 - mean_squared_error: 2.4053Epoch 00004: val_loss improved from 1.33298 to 1.27312, saving model to /almac/ignacio/40_20_50_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2258s - loss: 1.3161 - mean_absolute_error: 1.3161 - mean_squared_error: 2.3982 - val_loss: 1.2731 - val_mean_absolute_error: 1.2731 - val_mean_squared_error: 2.4572
Epoch 6/20
1040/1050 [============================>.] - ETA: 20s - loss: 1.2665 - mean_absolute_error: 1.2665 - mean_squared_error: 2.3540Epoch 00005: val_loss did not improve
1050/1050 [==============================] - 2514s - loss: 1.2675 - mean_absolute_error: 1.2675 - mean_squared_error: 2.3582 - val_loss: 1.5086 - val_mean_absolute_error: 1.5086 - val_mean_squared_error: 3.6239
Epoch 7/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.2228 - mean_absolute_error: 1.2228 - mean_squared_error: 2.2425Epoch 00006: val_loss did not improve
1050/1050 [==============================] - 2420s - loss: 1.2211 - mean_absolute_error: 1.2211 - mean_squared_error: 2.2343 - val_loss: 1.4865 - val_mean_absolute_error: 1.4865 - val_mean_squared_error: 3.7128
Epoch 8/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.2039 - mean_absolute_error: 1.2039 - mean_squared_error: 2.2305Epoch 00007: val_loss did not improve
1050/1050 [==============================] - 2151s - loss: 1.2026 - mean_absolute_error: 1.2026 - mean_squared_error: 2.2239 - val_loss: 1.3063 - val_mean_absolute_error: 1.3063 - val_mean_squared_error: 2.5354
Epoch 9/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.1820 - mean_absolute_error: 1.1820 - mean_squared_error: 2.1469Epoch 00008: val_loss improved from 1.27312 to 1.18658, saving model to /almac/ignacio/40_20_50_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2270s - loss: 1.1822 - mean_absolute_error: 1.1822 - mean_squared_error: 2.1460 - val_loss: 1.1866 - val_mean_absolute_error: 1.1866 - val_mean_squared_error: 2.1257
Epoch 10/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.1823 - mean_absolute_error: 1.1823 - mean_squared_error: 2.1822Epoch 00009: val_loss did not improve
1050/1050 [==============================] - 2159s - loss: 1.1788 - mean_absolute_error: 1.1788 - mean_squared_error: 2.1752 - val_loss: 1.1912 - val_mean_absolute_error: 1.1912 - val_mean_squared_error: 2.0940
Epoch 11/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.1712 - mean_absolute_error: 1.1712 - mean_squared_error: 2.1304Epoch 00010: val_loss did not improve
1050/1050 [==============================] - 2402s - loss: 1.1720 - mean_absolute_error: 1.1720 - mean_squared_error: 2.1286 - val_loss: 1.3327 - val_mean_absolute_error: 1.3327 - val_mean_squared_error: 2.3660
Epoch 12/20
1040/1050 [============================>.] - ETA: 18s - loss: 1.1360 - mean_absolute_error: 1.1360 - mean_squared_error: 2.0058Epoch 00011: val_loss improved from 1.18658 to 1.15046, saving model to /almac/ignacio/40_20_50_fastText_100_50_stacked.hdf5
1050/1050 [==============================] - 2226s - loss: 1.1396 - mean_absolute_error: 1.1396 - mean_squared_error: 2.0186 - val_loss: 1.1505 - val_mean_absolute_error: 1.1505 - val_mean_squared_error: 1.9125
Epoch 13/20
1040/1050 [============================>.] - ETA: 15s - loss: 1.1094 - mean_absolute_error: 1.1094 - mean_squared_error: 2.0072Epoch 00012: val_loss did not improve
1050/1050 [==============================] - 1992s - loss: 1.1168 - mean_absolute_error: 1.1168 - mean_squared_error: 2.0354 - val_loss: 1.1734 - val_mean_absolute_error: 1.1734 - val_mean_squared_error: 2.2065
Epoch 14/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.0855 - mean_absolute_error: 1.0855 - mean_squared_error: 1.9231Epoch 00013: val_loss did not improve
1050/1050 [==============================] - 1994s - loss: 1.0825 - mean_absolute_error: 1.0825 - mean_squared_error: 1.9119 - val_loss: 1.1967 - val_mean_absolute_error: 1.1967 - val_mean_squared_error: 2.3252
Epoch 15/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.0929 - mean_absolute_error: 1.0929 - mean_squared_error: 1.9838Epoch 00014: val_loss did not improve
1050/1050 [==============================] - 2170s - loss: 1.0899 - mean_absolute_error: 1.0899 - mean_squared_error: 1.9725 - val_loss: 1.2605 - val_mean_absolute_error: 1.2605 - val_mean_squared_error: 2.6510
Epoch 16/20
1040/1050 [============================>.] - ETA: 16s - loss: 1.0720 - mean_absolute_error: 1.0720 - mean_squared_error: 1.8606Epoch 00015: val_loss did not improve
1050/1050 [==============================] - 2063s - loss: 1.0701 - mean_absolute_error: 1.0701 - mean_squared_error: 1.8535 - val_loss: 1.2138 - val_mean_absolute_error: 1.2138 - val_mean_squared_error: 2.2479
Epoch 17/20
1040/1050 [============================>.] - ETA: 17s - loss: 1.0582 - mean_absolute_error: 1.0582 - mean_squared_error: 1.9274Epoch 00016: val_loss did not improve
1050/1050 [==============================] - 2215s - loss: 1.0554 - mean_absolute_error: 1.0554 - mean_squared_error: 1.9174 - val_loss: 1.1801 - val_mean_absolute_error: 1.1801 - val_mean_squared_error: 2.1650
Epoch 18/20
1040/1050 [============================>.] - ETA: 21s - loss: 1.0669 - mean_absolute_error: 1.0669 - mean_squared_error: 1.8840Epoch 00017: val_loss did not improve
1050/1050 [==============================] - 2609s - loss: 1.0653 - mean_absolute_error: 1.0653 - mean_squared_error: 1.8816 - val_loss: 1.2278 - val_mean_absolute_error: 1.2278 - val_mean_squared_error: 2.4876
Epoch 19/20
1040/1050 [============================>.] - ETA: 19s - loss: 1.0515 - mean_absolute_error: 1.0515 - mean_squared_error: 1.8557Epoch 00018: val_loss did not improve
1050/1050 [==============================] - 2390s - loss: 1.0483 - mean_absolute_error: 1.0483 - mean_squared_error: 1.8519 - val_loss: 1.2164 - val_mean_absolute_error: 1.2164 - val_mean_squared_error: 2.3298
Epoch 20/20
1040/1050 [============================>.] - ETA: 20s - loss: 1.0355 - mean_absolute_error: 1.0355 - mean_squared_error: 1.8238Epoch 00019: val_loss did not improve
1050/1050 [==============================] - 2682s - loss: 1.0363 - mean_absolute_error: 1.0363 - mean_squared_error: 1.8266 - val_loss: 1.1818 - val_mean_absolute_error: 1.1818 - val_mean_squared_error: 2.2516

Parameters:
---------------------
h_STATES=40
EPOCHS=20
DENSES=50
Representation=fastText
EMBEDDING_DIM=100
MAX_SEQUENCE_LENGTH=50
MODEL_TYPE=stacked